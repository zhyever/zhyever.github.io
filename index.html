<!DOCTYPE html>

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Zhenyu Li's home page"> 
  
  <link href="./files/wfdoc.css" rel="stylesheet" type="text/css"> 
  <title>Zhenyu Li's Homepage</title> 
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">

</head>


<script>
  function hideshow(x) {
    // var x = document.getElementById("toolbox_cite");
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }
</script>


<body> 
  <div id="layout-content" style="margin-top: 25px;">
  <table>
    <tbody>
    <tr>
      <td width="670">
        <div id="toptitle">
        <h1>Zhenyu Li &nbsp; 李震宇</h1></div>
        <h3>Phd Student</h3>
        <!-- <p>Advanced Imaging and Intelligent Analysis Laboratory -->
        <!-- <br>Computing Faculty -->
        <br>King Abdullah University of Science and Technology
        <br>
        <br> Email:
        <!-- <a href="zhenyuli17@hit.edu.cn"> zhenyuli17@hit.edu.cn</a>;
        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="zhenyu.li.9955@gmail.com"> zhenyu.li.9955@gmail.com</a>  -->
        <a href="zhenyu.li.9955@gmail.com"> zhenyu.li.9955@gmail.com</a>;
        <br> Github: 
        <a href="https://github.com/zhyever">https://github.com/zhyever</a> 
        <br> Google Scholar: 
        <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=RYjoFN4AAAAJ">Google Scholar Link</a> 
        <br> CV: 
        <a href="https://zhyever.github.io/files/ZhenyuLi_CV.pdf">CV Link</a> 
        <br><br></p>
      </td>
      <td>
        <img width="180" src="./files/zhenyuli.jpg" border="0">
      </td>
    </tr>
    <tr></tr></tbody>
  </table>
  <div id="layout-content" style="margin-top: 25px;">

    
<h2>Biography</h2>

<p> I'm a 2nd-year PhD student at King Abdullah University of Science and Technology (KAUST), advised by <a href="http://peterwonka.net/">Prof. Peter Wonka</a>. </p>
<p> I got my B.E. and M.S degrees in computer science at Harbin Institute of Technology, China. </p>
<p> My research currently concentrates on 3D reconstruction and scene understanding.</p>

<!-- <p> <b>Looking for an internship position in 2025 now.</b> </p> -->


<!-- <h2>Hobbies</h2>

<p> I am a <a href="https://www.redbull.com/int-en/videos/red-bull-bc-one-2022-world-final-teaser">breaking </a> dancer (b-boy). I led the breaking dance group in HIT Crew for two years. 
  My favorite stars include <a href="https://www.youtube.com/watch?v=Z2YgNJMz12A&ab_channel=RedBullBCOne">Cloud</a>, <a href="https://www.youtube.com/watch?v=w9Kyhjk6uSk&ab_channel=RedBullBCOne">Taisuke</a>, and <a href="https://www.youtube.com/watch?v=ObWzq8h0Wq0&ab_channel=stance">Issei</a>.
  I enjoy listening to music and flowing like rivers. Dancing makes me confident and not afraid to face challenges. You can see my freezes and povermoves [<a href="javascript:hideshow(document.getElementById('my_photos'))" >here</a>] (not a hyperlink). </p>

<p> Besides, I have a wide interest including but not limited to hiking, cooking, and basketball. </p>

<div id="my_photos" style="display:none">
  <table>
    <tr>
      <td><img width="180" src="./files/my_freeze_1.jpg" border="0"></td>
      <td>Nike (Air Jordan)</td>
    </tr>
    <tr>
      <td><img width="180" src="./files/my_freeze_2.jpg" border="0"></td>
      <td>Pilot Freeze</td>
    </tr>
    <tr>
      <td><img width="180" src="./files/my_powermove.gif" border="0"></td>
      <td>Windmill</td>
    </tr>
  </table>
</div> -->

<h2>News</h2>
<ul>
  <li>
    June 2025, <a href="https://github.com/zhyever/Amodal-Depth-Anything">Amodal-DepthAnything</a> is accepted to <b>ICCV 2025</b>! 
  </li>
  <!-- <li>
    January 2025, I started my internship at <a href="https://seed.bytedance.com/en">ByteDance Seed</a>!
  </li> -->
  <li>
    July 2024, <a href="https://github.com/zhyever/PatchRefiner">PatchRefiner</a> is accepted to <b>ECCV 2024</b>! 
  </li>
  <li>
    March 2024, <a href="https://zhyever.github.io/patchfusion/">PatchFusion</a> is accepted to <b>CVPR 2024</b>! 
  </li>
</ul>

<h2>Experience</h2>
<ul>
  <li>
    Mar.2021 - Sep.2021, <i>Development and Research Intern</i>, <b>SenseTime</b>
  </li>
  <li>
    Jan.2022 - July.2022, <i>Research Intern</i>, <b>SenseTime</b>
  </li>
  <li>
    Aug.2022 - Apr.2023, <i>Research Intern (Elite Camp)</i>, <b>DiDi Cargo</b>
  </li>
  <li>
    Jan.2025 - July.2025, <i>Research Intern (TopSeed Candidate)</i>, <b>ByteDance Seed</b>
  </li>  
</ul>

<h2>Awards</h2>
<ul>
  <li>
    <b>1<sup>st</sup></b> place at VCL 2023 Challenge, Multitask Learning for Robustness Track! (<b>ICCV 2023</b> Workshop)
  </li>
  <li>
    China National Scholarship 2022.
  </li>
  <li>
    <b>3<sup>rd</sup></b> place at SSLAD 2022 Challenge, 3D Object Detection Track! (<b>ECCV 2022</b> Workshop)
  </li>
  <li>
    <b>2<sup>nd</sup></b> place at Mobile AI&AIM 2022 Challenge, Monocular Depth Estimation Track! (<b>ECCV 2022</b> Workshop)
  </li>
</ul>

<h2>Codebase</h2>

<table class="codebase_table">
  <tbody>
  
  <tr>
    <td class="codebase_table_td1"><img src="./files/teaser.gif" class="papericon"></td>
      <td class="codebase_table_td2"><br><b>Monocular Depth Estimation Toolbox</b>
      <br><u>Zhenyu Li</u>
      <br>2022<br>
      [<a href="https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox">Code</a>] [<a href="javascript:hideshow(document.getElementById('toolbox_cite'))" >Bibtex</a>] <a href="https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox" target="_blank"> <img src="https://img.shields.io/github/stars/zhyever/Monocular-Depth-Estimation-Toolbox?style=social" alt="GitHub stars" style="height: 15px; vertical-align: -2px;"/> </a> 
    </td>
  </tr>

  <tr>
    <td class="codebase_table_td1" colspan="2">
    <div id="toolbox_cite" style="display:none">
      @misc{lidepthtoolbox2022,
        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Monocular Depth Estimation Toolbox},
        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Zhenyu Li},
        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;howpublished = {\url{https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox}},
        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2022}
        <br>}
    </div>
    </td>
  </tr>

  </tbody>
</table>

<h2>Selected Publications</h2>

<table class="pub_table">
  <tbody>

  <tr>
    <td class="pub_td1"><img src="./files/amodaldepth.png" class="papericon"></td>
      <td class="pub_td2"><br><b>Amodal Depth Anything: Amodal Depth Estimation in the Wild</b>
      <br><u>Zhenyu Li</u>, Mykola Lavreniuk, Shariq Farooq Bhat, Peter Wonka
      <br>ICCV, 2025
      <br>
      [<a href="https://arxiv.org/abs/2412.02336">PDF</a>]
      [<a href="https://github.com/zhyever/Amodal-Depth-Anything">Code</a>]
    </td>
  </tr>
  
  <tr>
    <td class="pub_td1"><img src="./files/patchrefiner.png" class="papericon"></td>
      <td class="pub_td2"><br><b>PatchRefiner: Leveraging Synthetic Data for Real-Domain High-Resolution Monocular Metric Depth Estimation</b>
      <br><u>Zhenyu Li</u>, Shariq Farooq Bhat, Peter Wonka
      <br>ECCV, 2024
      <br>
      [<a href="https://arxiv.org/abs/2406.06679">PDF</a>]
      [<a href="https://github.com/zhyever/PatchRefiner">Code</a>]
    </td>
  </tr>

    
  <tr>
    <td class="pub_td1"><img src="./files/patchfusion.gif" class="papericon"></td>
      <td class="pub_td2"><br><b>PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation</b>
      <br><u>Zhenyu Li</u>, Shariq Farooq Bhat, Peter Wonka
      <br>CVPR, 2024
      <br>
      [<a href="https://zhyever.github.io/patchfusion/">Project Page</a>]
      [<a href="https://arxiv.org/abs/2312.02284">PDF</a>]
      [<a href="https://github.com/zhyever/PatchFusion">Code</a>] <a href="https://github.com/zhyever/PatchFusion" target="_blank"> <img src="https://img.shields.io/github/stars/zhyever/PatchFusion?style=social" alt="GitHub stars" style="height: 15px; vertical-align: -2px;"/> </a>
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/stmono3d.png" class="papericon"></td>
      <td class="pub_td2"><br><b>Unsupervised Domain Adaptation for Monocular 3D Object Detection via Self-Training</b>
      <br><u>Zhenyu Li</u>, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang
      <br>ECCV, 2022
      <br>
      [<a href="https://arxiv.org/abs/2204.11590">PDF</a>]
      <!-- [<a href="https://github.com/zhyever/STMono3D">Code</a>] -->
    </td>
  </tr>

  <!-- <tr>
    <td class="pub_td1"><img src="./files/simipu.png" class="papericon"></td>
    <td class="pub_td2"><br><b>SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-Training for Spatial-Aware Visual Representations</b>
      <br><u>Zhenyu Li</u>, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang, Bolei Zhou, Hang Zhao
      <br>AAAI, 2022
      <br>
      [<a href="https://arxiv.org/abs/2112.04680">PDF</a>]
      [<a href="https://github.com/zhyever/SimIPU">Code</a>]
    </td>
  </tr> -->
 
  <tr>
    <td class="pub_td1"><img src="./files/autoalignv2.png" class="papericon"></td>
    <td class="pub_td2"><br><b>AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection</b>
      <br>Zehui Chen, <u>Zhenyu Li</u>, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao
      <br>ECCV, 2022<br>
      [<a href="https://arxiv.org/abs/2207.10316">PDF</a>]
      [<a href="https://github.com/zehuichen123/AutoAlignV2">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/autoalign.png" class="papericon"></td>
    <td class="pub_td2"><br><b>AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection</b>
      <br>Zehui Chen, <u>Zhenyu Li</u>, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao, Bolei Zhou, Hang Zhao
      <br>IJCAI, 2022<br>
      [<a href="https://arxiv.org/abs/2201.06493">PDF</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/binsformer.png" class="papericon"></td>
    <td class="pub_td2"><br><b>BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation</b>
      <br><u>Zhenyu Li</u>, Xuyang Wang, Xianming Liu, Junjun Jiang
      <br><font color="red">Ranked <b>1<sup>st</sup></b> on KITTI depth estimation benchmark (Feb, 2022).</font>
      <br>Transactions on Image Processing
      <br>
      [<a href="https://arxiv.org/abs/2204.00987">PDF</a>]
      [<a href="https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/depthformer.png" class="papericon"></td>
    <td class="pub_td2"><br><b>DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation</b>
      <br><u>Zhenyu Li</u>, Zehui Chen, Xianming Liu, Junjun Jiang
      <br><font color="red">Ranked <b>1<sup>st</sup></b> on KITTI depth estimation benchmark (Nov, 2021).</font>
      <br>Machine Intelligence Research
      <br>
      [<a href="https://arxiv.org/abs/2203.14211">PDF</a>]
      [<a href="https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox">Code</a>]
    </td>
  </tr>

  </tbody>
</table>


<h2>Service</h2>

<ul>
  <li> Conference Reviewer: CVPR, ECCV, ICCV, NeurIPS, SIGGRAPH.</li>
</ul>

<br>



<!-- <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=dUepM9XJyjoQI4HbN3FsGSefGSWir8zqauXaVl5-W1w&cl=ffffff&w=a"></script> -->
<!-- <a href="https://mapmyvisitors.com/web/1bvdv"  title="Visit tracker"><img src="https://mapmyvisitors.com/map.png?d=dUepM9XJyjoQI4HbN3FsGSefGSWir8zqauXaVl5-W1w&cl=ffffff" /></a> -->

</div></div></body></html>
